{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rospy\n",
    "from franka_cal_sim.srv import*\n",
    "import cv2\n",
    "from cv_bridge import CvBridge\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, GRU, Masking, LSTM\n",
    "from tensorflow.keras.layers import Add, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.compat.v1.keras import backend as K\n",
    "import datetime\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tempfile import TemporaryFile\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "def stack_samples(samples):\n",
    "    array = np.array(samples)\n",
    "\n",
    "    current_states = np.stack(array[:,0]).reshape((array.shape[0],-1,array[0,0].shape[2]))\n",
    "    current_act_hists = np.stack(array[:,1]).reshape((array.shape[0],-1,array[0,1].shape[2]))\n",
    "    actions = np.stack(array[:,2]).reshape((array.shape[0],-1))\n",
    "    rewards = np.stack(array[:,3]).reshape((array.shape[0],-1))\n",
    "    new_states = np.stack(array[:,4]).reshape((array.shape[0],-1,array[0,4].shape[2]))\n",
    "    new_act_hists = np.stack(array[:,5]).reshape((array.shape[0],-1,array[0,5].shape[2]))\n",
    "    hiddens = np.stack(array[:,6]).reshape((array.shape[0],array[0,6].shape[1]))\n",
    "    new_hiddens = np.stack(array[:,7]).reshape((array.shape[0],array[0,7].shape[1]))\n",
    "    dones = np.stack(array[:,8]).reshape((array.shape[0],1))\n",
    "\n",
    "    return current_states, current_act_hists, actions, rewards, new_states, new_act_hists, hiddens, new_hiddens, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-a5462b1f7afd>, line 163)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-a5462b1f7afd>\"\u001b[0;36m, line \u001b[0;32m163\u001b[0m\n\u001b[0;31m    cur_states, cur_act_hists, actions, rewards, new_states,new_act_hists,hiddens,new_hiddens _ =  stack_samples(samples)\u001b[0m\n\u001b[0m                                                                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class ModelBasedRL:\n",
    "    def __init__(self, obs_dim, act_dim, sess):\n",
    "        \n",
    "        #data flow:\n",
    "        #last_hidden, obs, last_act -> hidden_state\n",
    "        #trans: hidden_state, act -> next_obs\n",
    "        #reward: hidden_state, act -> reward\n",
    "        #actor: hidden_state -> new act\n",
    "        #critic: hidden_state, act -> Q\n",
    "        \n",
    "        ##session\n",
    "        self.sess = sess\n",
    "        \n",
    "        ##hyperparameter:\n",
    "        #learning rate\n",
    "        self.trans_lr = 1e-4 \n",
    "        self.reward_lr = 1e-4 \n",
    "        self.actor_lr = 1e-4 \n",
    "        self.critic_lr = 1e-4\n",
    "        \n",
    "        #training\n",
    "        self.batch_size = 64   \n",
    "        self.reg_w = 0.01\n",
    "        \n",
    "        #rl\n",
    "        self.gamma = .99\n",
    "        self.tau   = 0.01\n",
    "        self.num_step = 3\n",
    "        self.hidden_dim = 64\n",
    "        \n",
    "        #build models\n",
    "        #h: hidden model\n",
    "        #t: transition model\n",
    "        #r: reward model\n",
    "        #a: actor model\n",
    "        #c: critic model\n",
    "        self.obs_input, self.last_act_input,self.hidden_input,self.h_model = self.create_hidden_model()\n",
    "        self.t_action_input,self.t_model = self.create_trans_model()\n",
    "        self.r_action_input,self.r_model = self.create_reward_model()\n",
    "        self.a_model = self.create_actor_model()\n",
    "        self.c_model = self.create_critic_model()\n",
    "        \n",
    "        #target networks (mixed cases)\n",
    "        self.tgt_a_model = self.create_actor_model()\n",
    "        self.tgt_c_model = self.create_critic_model()\n",
    "        \n",
    "        #create actor loss\n",
    "        self.init_s_input,self.init_a_input,self.init_h_input, self.a_loss = self.create_actor_loss()\n",
    "        self.a_optimizer = tf.keras.optimizers.Adam(learning_rate=self.actor_lr)\n",
    "        self.a_opt_op = self.a_optimizer.minimize(self.a_loss,self.a_model.trainable_weights)\n",
    "        \n",
    "    def create_hidden_model(self):\n",
    "        # ========================================================================= #\n",
    "        #state, last hidden -> hidden\n",
    "        state_input = Input(shape=(None,self.obs_dim))\n",
    "        act_input = Input(shape=(None,self.act_dim))\n",
    "        hidden_input = Input(shape=(self.hidden_dim,))\n",
    "        hidden_rnn,state_h = GRU(self.hidden_dim, return_state=True, initial_state=hidden_input)(Concatenate()([state_input,act_input]))\n",
    "        \n",
    "        h_model  = Model([self.obs_input,self.last_act_input,self.hidden_input],state_h)\n",
    "\n",
    "        return state_input, act_input, hidden_input, h_model   \n",
    "        \n",
    "    def create_trans_model(self):\n",
    "        \n",
    "        #hidden,action -> next_obs\n",
    "        \n",
    "        t_action_input = Input(shape=self.act_dim)\n",
    "        t_action_h1    = Dense(500)(t_action_input)\n",
    "\n",
    "        t_merged    = Concatenate()([self.hidden_state, t_action_h1])\n",
    "        t_merged_h1 = Dense(500, activation='relu')(t_merged)\n",
    "        t_output = Dense(self.hidden_dim, activation='linear')(t_merged_h1)\n",
    "        t_model  = Model([self.obs_input,self.last_act_input,self.hidden_input,t_action_input],t_output)\n",
    "\n",
    "        adam  = Adam(lr=self.trans_lr)\n",
    "        t_model.compile(loss=\"mse\", optimizer=adam)\n",
    "        \n",
    "        return t_action_input, t_model\n",
    "        \n",
    "    \n",
    "    def create_reward_model(self):\n",
    "        \n",
    "        #hidden,action -> reward\n",
    "        \n",
    "        r_action_input = Input(shape=self.act_dim)\n",
    "        r_action_h1    = Dense(500)(r_action_input)\n",
    "\n",
    "        r_merged    = Concatenate()([self.hidden_state, r_action_h1])\n",
    "        r_merged_h1 = Dense(500, activation='relu')(r_merged)\n",
    "        r_output = Dense(1, activation='linear')(r_merged_h1)\n",
    "        r_model  = Model([self.obs_input,self.last_act_input,self.hidden_input,r_action_input],r_output)\n",
    "\n",
    "        adam  = Adam(lr=self.reward_lr)\n",
    "        r_model.compile(loss=\"mse\", optimizer=adam)\n",
    "        return r_action_input, r_model\n",
    "        \n",
    "        return t_model\n",
    "    \n",
    "    def create_actor_model(self):\n",
    "        # ========================================================================= #\n",
    "        #hidden -> action\n",
    "        a_h = Dense(500, activation='relu')(self.hidden_state)\n",
    "        a_output = Dense(self.act_dim, activation='tanh')(a_h)\n",
    "\n",
    "        a_model = Model([self.obs_input,self.last_act_input,self.hidden_input], a_output)\n",
    "        adam  = Adam(lr=self.actor_lr)\n",
    "        a_model.compile(loss=\"mse\", optimizer=adam)\n",
    "        \n",
    "        return a_model\n",
    "    \n",
    "    \n",
    "    def create_critic_model(self):\n",
    "        \n",
    "        #hidden,action -> reward\n",
    "        \n",
    "        c_action_input = Input(shape=self.act_dim)\n",
    "        c_action_h1    = Dense(500)(c_action_input)\n",
    "\n",
    "        c_merged    = Concatenate()([self.hidden_state, c_action_h1])\n",
    "        c_merged_h1 = Dense(500, activation='relu')(c_merged)\n",
    "        c_output = Dense(1, activation='linear')(c_merged_h1)\n",
    "        c_model  = Model([self.obs_input,self.last_act_input,self.hidden_input,c_action_input],c_output)\n",
    "\n",
    "        adam  = Adam(lr=self.critic_lr)\n",
    "        c_model.compile(loss=\"mse\", optimizer=adam)\n",
    "        \n",
    "        return c_action_input, c_model\n",
    "        \n",
    "        \n",
    "    def create_actor_loss(self):\n",
    "        #initial state, action input (fixed)\n",
    "        init_s_input = Input(shape=(None,self.obs_dim))\n",
    "        init_a_input = Input(shape=(None,self.act_dim))\n",
    "        init_h_input = Input(shape=(None,self.act_dim))\n",
    "        loss = 0\n",
    "        \n",
    "        #first loop\n",
    "        h_0 = self.h_model([init_s_input, init_a_input,init_h_input])\n",
    "        a_0 = self.a_model([init_s_input, init_a_input,init_h_input])\n",
    "        r_0 = self.r_model([init_s_input, init_a_input, init_h_input, a_0])\n",
    "        o_1 = self.t_model([init_s_input, init_a_input, init_h_input,a_0])\n",
    "    \n",
    "        loss -= r_0\n",
    "        \n",
    "        #second loop\n",
    "        h_1 = self.h_model([o_1, a_0, h_0])\n",
    "        a_1 = self.a_model([o_1, a_0, h_0])\n",
    "        r_1 = self.r_model([o_1, a_0, h_0, a_1])\n",
    "        o_2 = self.t_model([o_1, a_0, h_0, a_1])\n",
    "        \n",
    "        loss -= r_1\n",
    "        \n",
    "        #third loop\n",
    "        a_2 = self.a_model([o_2, a_1, h_1])\n",
    "        r_2 = self.r_model([o_2, a_1, h_1,a_2])\n",
    "        \n",
    "        loss -= r_2\n",
    "        \n",
    "        return init_s_input,init_a_input,init_h_input, loss\n",
    "    \n",
    "    def train_trans(self,samples):\n",
    "        cur_states, cur_act_hists, actions, rewards, new_states,new_act_hists,hiddens,new_hiddens _ =  stack_samples(samples)\n",
    "        \n",
    "        self.t_model.fit([cur_states,cur_act_hists,hiddens,actions],new_states)\n",
    "        \n",
    "    def train_reward(self,samples):\n",
    "        cur_states, cur_act_hists, actions, rewards, new_states,new_act_hists, _ =  stack_samples(samples)\n",
    "        self.r_model.fit([cur_states,cur_act_hists,hiddens,actions],rewards)\n",
    "        \n",
    "    def train_actor(self,samples):\n",
    "        init_obs = np.zeros((self.obs_dim))\n",
    "        init_act = np.zeros((self.act_dim))\n",
    "        init_h = np.zeros((self.hidden_dim))\n",
    "        \n",
    "        self.a_opt_op.run(feed_dict={\n",
    "            self.init_s_input:init_obs,\n",
    "            self.init_a_input:init_act,\n",
    "            self.init_h_input:init_h\n",
    "        })\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_replay(path,actor,obs_dim,action_dim):\n",
    "    f = open(path, \"r\")\n",
    "    obs, actions, rewards, next_obs, terminals = [],[],[],[],[]\n",
    "    for line in f:\n",
    "        cols = line.strip().split('\\t')\n",
    "        obs_s = cols[0].split(';')\n",
    "        obs_t = []\n",
    "        for obss in obs_s:\n",
    "            obs_t.append([float(v) for v in obss.split(',')])\n",
    "        obs_s_tp1 = cols[3].split(';')\n",
    "        obs_tp1 = []\n",
    "        for obss in obs_s_tp1:\n",
    "            obs_tp1.append([float(v) for v in obss.split(',')])\n",
    "        obs_t = np.array(obs_t)\n",
    "        obs.append(obs_t)\n",
    "        action = np.array([float(v) for v in cols[1].split(',')])\n",
    "        actions.append(action)\n",
    "        rewards.append(float(cols[2]))\n",
    "        obs_tp1 = np.array(obs_tp1)\n",
    "        next_obs.append(obs_tp1)\n",
    "        terminals.append(bool(cols[4]==\"True\"))\n",
    "\n",
    "    print(terminals)\n",
    "    i = np.argmax(rewards)\n",
    "    print(i)\n",
    "    print(obs[i])\n",
    "    print(actions[i])\n",
    "    print(rewards[i])\n",
    "    plt.plot(np.linalg.norm(actions,axis=1))\n",
    "    plt.show()\n",
    "    plt.plot(rewards)\n",
    "    plt.show()\n",
    "\n",
    "    #save data\n",
    "    for i in range(len(obs)):\n",
    "        obs_action_seq = obs[i]\n",
    "        obs_seq = obs_action_seq[:,0:12]\n",
    "        ########\n",
    "        #obs_seq = np.zeros((6,12))\n",
    "        act_seq = obs_action_seq[:,12:48]\n",
    "        #print(obs_seq)\n",
    "        #print(act_seq)\n",
    "        num_l = np.sum(abs(actions[i])>0.018)\n",
    "        action = actions[i]\n",
    "        if np.linalg.norm(action)<1:\n",
    "            action = actions[i]/0.02\n",
    "\n",
    "\n",
    "\n",
    "        reward = rewards[i]\n",
    "        #reward-=num_l*0.5\n",
    "\n",
    "        next_obs_action_seq = next_obs[i]\n",
    "        next_obs_seq = next_obs_action_seq[:,0:12]\n",
    "        next_act_seq = next_obs_action_seq[:,12:48]\n",
    "        obs_seq = obs_seq.reshape((1, -1, obs_dim))\n",
    "        act_seq = act_seq.reshape((1, -1, action_dim))\n",
    "        #next_obs_seq = np.zeros((6,12))\n",
    "        next_obs_seq = next_obs_seq.reshape((1, -1, obs_dim))\n",
    "        ########\n",
    "        next_act_seq = next_act_seq.reshape((1, -1, action_dim))\n",
    "        done = terminals[i]\n",
    "        actor.remember(obs_seq, act_seq, action, reward, next_obs_seq, next_act_seq, done)\n",
    "        #augment data\n",
    "        next_act_seq_1 = np.concatenate([act_seq.reshape(((-1,action_dim)))[1:],-action.reshape((1,action_dim))],axis = 0).reshape((1, -1, action_dim))\n",
    "        next_act_seq_2 = np.concatenate([-act_seq.reshape(((-1,action_dim)))[1:],action.reshape((1,action_dim))],axis = 0).reshape((1, -1, action_dim))\n",
    "        actor.remember(obs_seq, -act_seq, -action, reward, next_obs_seq, -next_act_seq, done)\n",
    "        actor.remember(obs_seq, act_seq, -action, reward, next_obs_seq, next_act_seq_1, done)\n",
    "        actor.remember(obs_seq, -act_seq, action, reward, next_obs_seq, next_act_seq_2, done)\n",
    "\n",
    "    print(len(obs))\n",
    "\n",
    "\n",
    "def normalize(v):\n",
    "    norm = np.linalg.norm(v)\n",
    "    if norm == 0:\n",
    "        return v\n",
    "    return v / norm\n",
    "\n",
    "def step(state, action):\n",
    "    rospy.wait_for_service('/model_client/rl_service')\n",
    "    rospy.loginfo(rospy.get_caller_id() + 'begin service')\n",
    "    take_step = rospy.ServiceProxy('/model_client/rl_service', RLSrv)\n",
    "    resp1 = take_step(0, state.tolist(), action.tolist())\n",
    "    return resp1.next_state, resp1.reward, resp1.done\n",
    "\n",
    "def reset():\n",
    "    rospy.wait_for_service('/model_client/rl_service')\n",
    "    rospy.loginfo(rospy.get_caller_id() + 'begin service')\n",
    "    reset_step = rospy.ServiceProxy('/model_client/rl_service', RLSrv)\n",
    "    resp1 = reset_step(1, [], [])\n",
    "    return resp1.next_state, resp1.reward, resp1.done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation():\n",
    "    #record the reward trend\n",
    "    reward_list = []\n",
    "    test_reward_list = []\n",
    "\n",
    "    #save path:\n",
    "    # actor_checkpoint = rospy.get_param(\"/rl_client/actor_checkpoint\")\n",
    "    # critic_checkpoint = rospy.get_param(\"/rl_client/critic_checkpoint\")\n",
    "    # fig_path = rospy.get_param(\"/rl_client/figure_path\")\n",
    "    # result_path = rospy.get_param(\"/rl_client/result_path\")\n",
    "    # test_result_path = rospy.get_param(\"/rl_client/test_result_path\")\n",
    "\n",
    "    #init\n",
    "    obs_dim = 12\n",
    "    action_dim = 36\n",
    "    sess = tf.compat.v1.Session()\n",
    "    K.set_session(sess)\n",
    "    actor_critic = ActorCritic(sess)\n",
    "\n",
    "    path = \"/home/yunke/prl_proj/panda_ws/src/franka_cal_sim/python/replay_buffers/replay_buffer_imu_final.txt\"\n",
    "    #read replay buffer\n",
    "    read_replay(path,actor_critic,obs_dim,action_dim)\n",
    "\n",
    "    path = \"/home/yunke/prl_proj/panda_ws/src/franka_cal_sim/python/replay_buffer_last.txt\"\n",
    "    #read replay buffer\n",
    "    read_replay(path,actor_critic,obs_dim,action_dim)\n",
    "\n",
    "\n",
    "\n",
    "    path = \"/home/yunke/prl_proj/panda_ws/src/franka_cal_sim/python/replay_buffers/replay_buffer_tiny.txt\"\n",
    "\n",
    "    read_replay(path,actor_critic,obs_dim,action_dim)\n",
    "\n",
    "\n",
    "    num_trials = 10000\n",
    "    trial_len  = 3\n",
    "\n",
    "    for i in range(num_trials):\n",
    "\n",
    "        # reset()\n",
    "        # cur_state = np.ones(obs_dim)*500\n",
    "        # reward_sum = 0\n",
    "        # obs_list = []\n",
    "        # act_list = []\n",
    "        # cur_state = cur_state.reshape((1,obs_dim))\n",
    "        # obs_list.append(normalize(cur_state))\n",
    "        # act_list.append(normalize(0.01*np.ones((1,action_dim))))\n",
    "        # for j in range(trial_len):\n",
    "        #     #env.render()\n",
    "        #     print(\"trial:\" + str(i))\n",
    "        #     print(\"step:\" + str(j))\n",
    "        #     obs_seq = np.asarray(obs_list)\n",
    "        #     print(\"obs_seq\"+str(obs_seq))\n",
    "        #     act_seq = np.asarray(act_list)\n",
    "        #     obs_seq = obs_seq.reshape((1, -1, obs_dim))\n",
    "        #     act_seq = act_seq.reshape((1, -1, action_dim))\n",
    "        #     action = actor_critic.act(obs_seq,act_seq)\n",
    "        #     action = action.reshape((action_dim))\n",
    "        #     cur_state = cur_state.reshape(obs_dim)\n",
    "        #     new_state, reward, done = step(cur_state,action)\n",
    "        #     reward_sum += reward\n",
    "        #     rospy.loginfo(rospy.get_caller_id() + 'got reward %s',reward)\n",
    "        #     if j == (trial_len - 1):\n",
    "        #         done = True\n",
    "\n",
    "\n",
    "\n",
    "            actor_critic.train(i)\n",
    "            actor_critic.update_target()\n",
    "            obs_list = []\n",
    "            act_list = []\n",
    "            cur_state = np.zeros(obs_dim)\n",
    "            cur_state = cur_state.reshape((1,obs_dim))\n",
    "            obs_list.append(cur_state)\n",
    "            act_list.append(0.01*np.ones((1,action_dim)))\n",
    "            obs_seq = np.asarray(obs_list)\n",
    "            act_seq = np.asarray(act_list)\n",
    "            obs_seq = obs_seq.reshape((1, -1, obs_dim))\n",
    "            act_seq = act_seq.reshape((1, -1, action_dim))\n",
    "            action = actor_critic.actor_model.predict([obs_seq,act_seq])*0.02\n",
    "            if i%200==0:\n",
    "                print(action[0])\n",
    "                print(np.linalg.norm(action[0]))\n",
    "                action = np.ones((1,action_dim))\n",
    "                print(actor_critic.critic_model.predict([obs_seq,act_seq,action]))\n",
    "                action = np.ones((1,action_dim))*0.05\n",
    "                act_seq = np.ones((1,1,action_dim))*0.001\n",
    "                print(actor_critic.critic_model.predict([obs_seq,act_seq,action]))\n",
    "\n",
    "            # new_state = np.asarray(new_state).reshape((1,obs_dim))\n",
    "            # action = action.reshape((1,action_dim))\n",
    "\n",
    "            # obs_list.append(normalize(new_state))\n",
    "            # act_list.append(normalize(action))\n",
    "            # next_obs_seq = np.asarray(obs_list)\n",
    "            # next_act_seq = np.asarray(act_list)\n",
    "            # next_obs_seq = next_obs_seq.reshape((1, -1, obs_dim))\n",
    "            # next_act_seq = next_act_seq.reshape((1, -1, action_dim))\n",
    "\n",
    "            # #padding\n",
    "            # pad_width = trial_len-np.size(obs_seq,1)\n",
    "            # rospy.loginfo(rospy.get_caller_id() + 'obs_shape %s',obs_seq.shape)\n",
    "            # obs_seq = np.pad(obs_seq,((0,0),(pad_width,0),(0,0)),'constant')\n",
    "            # next_obs_seq = np.pad(next_obs_seq,((0,0),(pad_width,0),(0,0)),'constant')\n",
    "            # act_seq = np.pad(act_seq,((0,0),(pad_width,0),(0,0)),'constant')\n",
    "            # next_act_seq = np.pad(next_act_seq,((0,0),(pad_width,0),(0,0)),'constant')\n",
    "            # #print(obs_seq.shape)\n",
    "            # #print(next_obs_seq.shape)\n",
    "\n",
    "            # actor_critic.remember(obs_seq, act_seq, action, reward, next_obs_seq, next_act_seq, done)\n",
    "            # cur_state = new_state\n",
    "            # if done:\n",
    "            #     rospy.loginfo(rospy.get_caller_id() + 'got total reward %s',reward_sum)\n",
    "            #     reward_list.append(reward_sum)\n",
    "            #     break\n",
    "\n",
    "        # if i % 5 == 0:\n",
    "        #     actor_critic.actor_model.save_weights(actor_checkpoint)\n",
    "        #     actor_critic.critic_model.save_weights(critic_checkpoint)\n",
    "        #     fig, ax = plt.subplots()\n",
    "        #     ax.plot(reward_list)\n",
    "        #     fig.savefig(fig_path)\n",
    "        #     np.savetxt(result_path, reward_list, fmt='%f')\n",
    "        #\n",
    "    #draw a graph\n",
    "    obs_list = []\n",
    "    act_list = []\n",
    "    cur_state = np.zeros(obs_dim)\n",
    "    cur_state = cur_state.reshape((1,obs_dim))\n",
    "    obs_list.append(cur_state)\n",
    "    act_list.append(0.01*np.ones((1,action_dim)))\n",
    "    obs_seq = np.asarray(obs_list)\n",
    "    act_seq = np.asarray(act_list)\n",
    "    obs_seq = obs_seq.reshape((1, -1, obs_dim))\n",
    "    act_seq = act_seq.reshape((1, -1, action_dim))\n",
    "    q = []\n",
    "    t_q = []\n",
    "    for i in range(100):\n",
    "        action = 0.015*np.ones((1,action_dim))*i\n",
    "        q_value = actor_critic.critic_model.predict([obs_seq,act_seq,action])\n",
    "        t_q_value = actor_critic.target_critic_model.predict([obs_seq,act_seq,action])\n",
    "        t_q.append(t_q_value.reshape((-1,)))\n",
    "        q.append(q_value.reshape((-1,)))\n",
    "\n",
    "    plt.plot(q)\n",
    "    plt.show()\n",
    "    plt.plot(t_q)\n",
    "    plt.show()\n",
    "    # #test\n",
    "    # rospy.init_node('RL_client', anonymous=True)\n",
    "    # reset()\n",
    "    # cur_state = np.zeros(obs_dim)\n",
    "    # test_reward_sum = 0\n",
    "    # test_reward_sum = 0\n",
    "    # obs_list = []\n",
    "    # act_list = []\n",
    "    # cur_state = cur_state.reshape((1,obs_dim))\n",
    "    # obs_list.append(cur_state)\n",
    "    # act_list.append(0.01*np.ones((1,action_dim)))\n",
    "    # for j in range(trial_len):\n",
    "    #     #env.render()\n",
    "    #     print(\"test:\" + str(i))\n",
    "    #     print(\"step:\" + str(j))\n",
    "    #\n",
    "    #     obs_seq = np.asarray(obs_list)\n",
    "    #     print(\"obs_seq:\" + str(obs_seq))\n",
    "    #     act_seq = np.asarray(act_list)\n",
    "    #     #obs_seq = np.zeros((6,12))\n",
    "    #     obs_seq = obs_seq.reshape((1, -1, obs_dim))\n",
    "    #\n",
    "    #     act_seq = act_seq.reshape((1, -1, action_dim))\n",
    "    #     action = actor_critic.actor_model.predict([obs_seq,act_seq])*0.02\n",
    "    #     action = action.reshape((action_dim))\n",
    "    #     print(action)\n",
    "    #     cur_state = cur_state.reshape(obs_dim)\n",
    "    #     new_state, reward, done = step(cur_state,action)\n",
    "    #     test_reward_sum += reward\n",
    "    #     rospy.loginfo(rospy.get_caller_id() + 'got reward %s',reward)\n",
    "    #     if j == (trial_len - 1):\n",
    "    #         done = True\n",
    "    #\n",
    "    #\n",
    "    #     new_state = np.asarray(new_state).reshape((1,obs_dim))\n",
    "    #     action = action.reshape((1,action_dim))\n",
    "    #\n",
    "    #     obs_list.append(new_state)\n",
    "    #     act_list.append(action)\n",
    "    #     cur_state = new_state\n",
    "    #     if done:\n",
    "    #         rospy.loginfo(rospy.get_caller_id() + 'got total reward %s',test_reward_sum)\n",
    "    #         test_reward_list.append(test_reward_sum)\n",
    "    #         # fig1, ax1 = plt.subplots()\n",
    "    #         # ax1.plot(test_reward_list)\n",
    "    #         #fig1.savefig('/home/yunke/prl_proj/panda_ws/src/franka_cal_sim/python/test_reward.png')\n",
    "    #         #np.savetxt(test_result_path, test_reward_list, fmt='%f')\n",
    "    #         break\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    simulation()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
